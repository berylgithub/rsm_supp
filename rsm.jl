using JLD, SparseArrays, Distributions, Statistics, StatsBase, ForwardDiff, ReverseDiff, LinearOperators, Krylov
using Random, Combinatorics
using ThreadsX

"""
========================================================================================================================================================================================================================
main engines
========================================================================================================================================================================================================================
"""

"""
The Bspline works for matrices

bspline constructor (mimic of matlab ver by prof. Neumaier)
params:
    - z, feature matrix (ndata, nfeature)
"""
function bspline(z)
    m, n = size(z)
    Î² = sparse(zeros(m, n))
    z = abs.(z)
    ind = (z .< 1)
    z1 = z[ind]
    Î²[ind] = 1 .+ 0.75*z1.^2 .*(z1 .- 2)
    ind = (.!ind) .&& (z .< 2)
    z1 = z[ind]
    Î²[ind] = 0.25*(2 .- z1).^3
    return Î²
end

"""
mostly unused (although faster), verbose version
"""
function bspline2(x)
    m, n = size(x) # fingerprint x data
    Î² = sparse(zeros(m, n))
    for j âˆˆ 1:n 
        for i âˆˆ 1:m
            z = abs(x[i,j])
            if z < 1
                Î²[i,j] = 1 + .75*x[i,j]^2 * (z - 2)
            elseif 1 â‰¤ z < 2
                Î²[i,j] = 0.25 * (2 - z)^3
            end
        end
    end
    return Î²
end

"""
Bspline but assumes the input is a scalar, for efficient AD purpose
"""
function bspline_scalar(x)
    Î² = 0.
    z = abs(x)
    if z < 1
        Î² = 1 + .75*x^2 * (z - 2)
    elseif 1 â‰¤ z < 2
        Î² = .25*(2-z)^3
    end
    return Î²
end

"""
wrapper to extract M+3 or n_basis amount of splines
params:
    - x, matrix, âˆˆ Float64 (n_features, n_data) 
    - M, number of basfunc, returns M+3 basfunc
outputs:
    - S, array of basfuncs
        if flatten âˆˆ Float64 (n_feature*(M+3), n_data)
        else âˆˆ Float64 (n_feature, n_data, M+3)
"""
function extract_bspline(x, M; flatten=false)
    n_feature, n_data = size(x)
    n_basis = M+3
    S = zeros(n_feature, n_data, n_basis)
    for i âˆˆ 1:M+3
        S[:, :, i] = bspline(M .* x .+ 2 .- i) # should be M+3 features
    end
    if flatten # flatten the basis
        S = permutedims(S, [1,3,2])
        S = reshape(S, n_feature*n_basis, n_data)
    end
    return S
end

"""
extract both Ï• and dÏ•
"""
function extract_bspline_df(x, M; flatten=false, sparsemat=false)
    n_feature, n_data = size(x)
    n_basis = M+3
    if flatten # flatten the basis
        rsize = n_feature*n_basis
        S = zeros(rsize, n_data)
        dÏ• = zeros(rsize, n_data)
        @simd for i âˆˆ 1:n_data
            rcount = 1
            @simd for j âˆˆ 1:n_basis
                @simd for k âˆˆ 1:n_feature
                    @inbounds S[rcount, i] = bspline_scalar(M*x[k, i] + 2 - j)
                    @inbounds dÏ•[rcount, i] = f_dÏ•(M*x[k, i] + 2 - j)
                    rcount += 1
                end
            end
        end
        if sparsemat # naive sparse, could still do smart sparse using triplets (I, J, V)
            S = sparse(S)
            dÏ• = sparse(dÏ•)
        end
    else # basis in last index of the array, possible for sparse matrix!!
        S = zeros(n_feature, n_data, n_basis)
        dÏ• = zeros(n_feature, n_data, n_basis)
        @simd for i âˆˆ 1:n_basis
            @simd for j âˆˆ 1:n_data
                @simd for k âˆˆ 1:n_feature
                    @inbounds S[k, j, i] = bspline_scalar(M*x[k, j] + 2 - i) # should be M+3 features
                    @inbounds dÏ•[k, j, i] = f_dÏ•(M*x[k, j] + 2 - i)
                end
            end
        end
    end
    return S, dÏ•
end

"""
tis uses sparse logic with (I, J, V) triplets hence it will be much more efficient.
"""
function extract_bspline_sparse(x, M; flatten=false)
    n_feature, n_data = size(x)
    n_basis = M+3
    if flatten # flatten the basis
        rsize = n_feature*n_basis
        S = zeros(rsize, n_data)
        dÏ• = zeros(rsize, n_data)
        @simd for i âˆˆ 1:n_data
            rcount = 1
            @simd for j âˆˆ 1:n_basis
                @simd for k âˆˆ 1:n_feature
                    @inbounds S[rcount, i] = bspline_scalar(M*x[k, i] + 2 - j)
                    @inbounds dÏ•[rcount, i] = f_dÏ•(M*x[k, i] + 2 - j)
                    rcount += 1
                end
            end
        end
    end
    #......
end


"""
wrapper for scalar w for Ï•'(w) = dÏ•(w)/dw
"""
function f_dÏ•(x)
    return ForwardDiff.derivative(bspline_scalar, x)
end

"""
Ï•'(w) = dÏ•(w)/dw using AD
params:
    - w, vector of features for a selected data, âˆˆ Float64 (n_feature) 
output:
    - y := Ï•'(w) âˆˆ Float64 (n_feature)
"""
function f_dÏ•_vec(w)
    y = similar(w)
    for i âˆˆ eachindex(w)
        y[i] = ForwardDiff.derivative(bspline_scalar, w[i])
    end
    return y
end

"""
more "accurate" basis extractor to the formula: 
    Ï•_l(w) := Î²_Ï„((Pw)_t), l = (Ï„, t), where Ï„ is the basis index, and t is the feature index, P is a scaler matrix (for now I with size of w)
"""
function Î²_Ï„(P, w)
    
end

"""
query for
Ï•(w[m], w[k])[l] = Ï•(w[m])[l] - Ï•(w[k])[l] - Ï•'(w[k])[l]*(w[m] - w[k]) is the correct one; Ï•'(w)[l] = dÏ•(w)[l]/dw,
currently uses the assumption of P = I hence P*w = w, the most correct one is Î²':= bspline'((P*w)_t), 
params:
    - l here corresponds to the feature index,
        used also to determine t, which is t = l % n_feature 
        *** NOT NEEDED ANYMORE ***if there exists B basis, hence there are M Ã— l indices, i.e., do indexing of l for each b âˆˆ B, the indexing formula should be: |l|(b-1)+l, where |l| is the feature length
    - Ï•, basis matrix, âˆˆ Float64(n_s := n_feature*n_basis, n_data), arranged s.t. [f1b1, f2b1, ...., fnbn]
    - dÏ•, the derivative of Ï•, idem to Ï•
    - W, feature matrix, âˆˆ Float64(n_feature, n_data)
    - m, index of selected unsup data
    - k, ... sup data 
optional:
    - force âˆˆ bool, whether to include the force condition or not, this is useful for equilibrium geometries
"""
function qÏ•(Ï•, dÏ•, W, m, k, l, n_feature; force=true)
    if force
        t = l % n_feature # find index t given index l and length of feature vector chosen (or n_f = L/n_b)
        if t == 0
            t = n_feature
        end
        return Ï•[l,m] - Ï•[l, k] - dÏ•[l, k]*(W[t,m]-W[t,k]) # Ï•_{kl}(w_m) := Ï•_l(w_m) - Ï•_l(w_k) - Ï•_l'(w_k)(w_m - w_k), for k âˆˆ K, l = 1,...,L 
    else
        return Ï•[l,m] - Ï•[l, k]
    end
end


"""
for (pre-)computing Ï•_{kl}(w_m) := Ï•_l(w_m) - Ï•_l(w_k) - Ï•_l'(w_k)(w_m - w_k), for k âˆˆ K (or k = 1,...,M), l = 1,...,L, m âˆˆ Widx, 
compute (and sparsify outside) B := B_{m,kl}, this is NOT a contiguous matrix hence it is indexed by row and column counter
instead of directly m and kl.
params mostly same as qÏ•
"""
function comp_B!(B, Ï•, dÏ•, W, Midx, Widx, L, n_feature)
    klc = 1                                                     # kl counter
    for k âˆˆ Midx
        for l âˆˆ 1:L
            rc = 1                                              # the row entry is not contiguous
            for m âˆˆ Widx
                B[rc, klc] = qÏ•(Ï•, dÏ•, W, m, k, l, n_feature) 
                rc += 1
            end
            klc += 1
        end
    end
end

"""
parallel version of B matrix computation, this allocates to memory inside the function, however it will benefit by the speedup if the CPU count is large 
"""
function comp_Bpar(Ï•, dÏ•, W, Midx, Widx, L, n_feature; force=true)
    itcol = Iterators.product(1:L, Midx) # concat block column indices
    its = Iterators.product(Widx, collect(itcol)[:]) # concat indices: row vector with vector form of itcol
    return ThreadsX.map(t-> qÏ•(Ï•, dÏ•, W, t[1], t[2][2], t[2][1], n_feature; force=force), its)
end

"""
kl index computer, which indexes the column of B
params:
    - M, number of sup data
    - L, n_feature*n_basis
"""
function kl_indexer(M, L)
    klidx = Vector{UnitRange}(undef, M) # this is correct, this is the kl indexer!!
    c = 1:M
    for i âˆˆ c
        n = (i-1)*L + 1 
        klidx[i] = n:n+L-1
    end
    return klidx
end


"""
compute S_K := âˆ‘1/Dâ‚–
params:
    - D, mahalanobis distance matrix, âˆˆ Float64 (n_data, n_data)
    - m, index of the selected unsupervised datapoint
    - Midx, list of index of supervised datapoints, âˆˆ Vector{Int64}
"""
function comp_SK(D, Midx, m)
    sum = 0.
    for i âˆˆ eachindex(Midx)
        sum += 1/D[m, i] # not the best indexing way...
    end
    return sum
end


function comp_Î³k(Dk, SK)
    return Dk*SK
end

function comp_Î±j(Dj, SK)
    return Dj*SK - 1
end

"""
returns a matrix with size m Ã— j
params:
    SKs, precomputed SK vector âˆˆ Float64(N)
"""
function comp_Î³(D, SKs, Midx, Widx)
    M = length(Midx); N = length(Widx)
    Î³ = zeros(N, M)
    for kc âˆˆ eachindex(Midx)
        for mc âˆˆ eachindex(Widx)
            m = Widx[mc]
            Î³[mc, kc] = D[m, kc]*SKs[mc]
        end
    end
    return Î³
end

"""
assemble A matrix and b vector for the linear system, should use sparse logic (I, J, V triplets) later!!
params:
    - W, data Ã— feature matrix, Float64 (n_feature, n_data)
    ...
"""
function assemble_Ab(W, E, D, Ï•, dÏ•, Midx, Widx, n_feature, n_basis)
    # assemble A (try using sparse logic later!!):
    n_m = length(Midx)
    n_w = length(Widx) # different from n_data!! n_data := size(W)[2]
    n_l = n_feature*n_basis
    rows = n_w*n_m
    cols = n_l*n_m
    A = zeros(rows, cols) 
    b = zeros(rows) 
    rcount = 1 #rowcount
    for m âˆˆ Widx
        SK = comp_SK(D, Midx, m)
        for j âˆˆ Midx
            ccount = 1 # colcount
            âˆ‘k = 0. # for the 2nd term of b
            Î±j = SK*D[j,m] - 1
            for k âˆˆ Midx
                Î³k = SK*D[k, m]
                den = Î³k*Î±j
                âˆ‘k = âˆ‘k + E[k]/den # E_k/(Î³k Ã— Î±j)
                for l âˆˆ 1:n_l # from flattened feature
                    Ï•kl = qÏ•(Ï•, dÏ•, W, m, k, l, n_feature)
                    #display(Ï•kl)
                    num = Ï•kl*(1-Î³k*Î´(j, k)) # see RoSemi.pdf and RSI.pdf for Ï• and dÏ• definition
                    A[rcount, ccount] = num/den
                    ccount += 1 # end of column loop
                end
            end
            b[rcount] = E[j]/Î±j - âˆ‘k # assign b vector elements
            rcount += 1 # end of row loop
        end
    end
    return A, b
end

"""
assemble A matrix and b vector for the linear system, with sparse logic (I, J, V triplets), dynamic vectors at first, 3x slower than static ones though
params:
    ...
"""
function assemble_Ab_sparse(W, E, D, Ï•, dÏ•, Midx, Widx, n_feature, n_basis)
    n_m = length(Midx)
    n_w = length(Widx) # different from n_data!! n_data := size(W)[2]
    n_l = n_feature*n_basis
    rows = n_w*n_m
    cols = n_l*n_m
    b = zeros(rows)
    J = Vector{Int64}(undef, 0); K = Vector{Int64}(undef, 0); V = Vector{Float64}(undef, 0); # empty vectors
    rcount = 1 #rowcount
    for m âˆˆ Widx
        SK = comp_SK(D, Midx, m)
        for j âˆˆ Midx
            ccount = 1 # colcount
            âˆ‘k = 0. # for the 2nd term of b
            Î±j = SK*D[j,m] - 1
            for k âˆˆ Midx
                Î³k = SK*D[k, m]
                den = Î³k*Î±j
                âˆ‘k = âˆ‘k + E[k]/den # E_k/(Î³k Ã— Î±j)
                for l âˆˆ 1:n_l # from flattened feature
                    Ï•kl = qÏ•(Ï•, dÏ•, W, m, k, l, n_feature)
                    #display(Ï•kl)
                    num = Ï•kl*(1-Î³k*Î´(j, k)) # see RoSemi.pdf and RSI.pdf for Ï• and dÏ• definition
                    val = num/den
                    # assign the vectors:
                    if val != 0. # check if it's nonzero then push everything
                        push!(J, rcount)
                        push!(K, ccount)
                        push!(V, val)
                    else
                        if (rcount == rows) && (ccount == cols) # final entry trick, push zeros regardless
                            push!(J, rcount)
                            push!(K, ccount)
                            push!(V, val)
                        end
                    end
                    ccount += 1 # end of column loop
                end
            end
            b[rcount] = E[j]/Î±j - âˆ‘k # assign b vector elements
            rcount += 1 # end of row loop
        end
    end
    A = sparse(J, K, V)
    return A, b
end


"""
predict the energy of w_m by computing V_K(w_m), naive or fair(?) version, since all quantities except phi are recomputed
params:
    - W, fingerprint matrix, âˆˆFloat64(n_feature, n_data)
    - ...
    - m, index of W in which we want to predict the energy
    - n_l := n_basis*n_feature, length of the feature block vector,
output:
    - VK, scalar Float64
notes:
    - for m = k, this returns undefined or NaN by definition of V_K(w).
"""
function comp_VK(W, E, D, Î¸, Ï•, dÏ•, Midx, n_l, n_feature, m)
    SK = comp_SK(D, Midx, m) # SK(w_m)
    RK = 0.
    ccount = 1 # the col vector count, should follow k*l, easier to track than trying to compute the indexing pattern.
    for k âˆˆ Midx
        âˆ‘l = 0. # right term with l index
        for l âˆˆ 1:n_l # âˆ‘Î¸_kl*Ï•_kl
            Ï•kl = qÏ•(Ï•, dÏ•, W, m, k, l, n_feature)
            #kl_idx = n_l*(k-1) + l # use the indexing pattern # doesnt work if the index is contiguous
            Î¸kl = Î¸[ccount] #Î¸[kl_idx]  # since Î¸ is in block vector of [k,l]
            âˆ‘l = âˆ‘l + Î¸kl*Ï•kl
            #println([ccount, Î¸kl, Ï•kl, âˆ‘l])
            ccount += 1
        end
        vk = E[k] + âˆ‘l
        RK = RK + vk/D[k, m] # D is symm
        #println([E[k], âˆ‘l, D[k, m], RK])
    end
    return RK/SK
end

"""
compute Î”_jK(w_m). Used for MAD and RMSD. See comp_VK function, since Î”_jK(w_m) := (VK - Vj)/Î±j
"""
function comp_Î”jK(W, E, D, Î¸, Ï•, dÏ•, Midx, n_l, n_feature, m, j; return_vk = false)
    SK = comp_SK(D, Midx, m) # compute SK
    RK = 0.
    âˆ‘l_j = 0. # for j indexer, only passed once and j âˆˆ K
    ccount = 1 # the col vector count, should follow k*l, easier to track than trying to compute the indexing pattern.
    for k âˆˆ Midx
        âˆ‘l = 0. # right term with l index
        for l âˆˆ 1:n_l # âˆ‘Î¸_kl*Ï•_kl
            # for k:
            Ï•kl = qÏ•(Ï•, dÏ•, W, m, k, l, n_feature)
            Î¸kl = Î¸[ccount] # since Î¸ is in block vector of [k,l]
            Î¸Ï• = Î¸kl*Ï•kl
            âˆ‘l = âˆ‘l + Î¸Ï•
            if k == j # for j terms:
                âˆ‘l_j = âˆ‘l_j + Î¸Ï•
            end
            #println([ccount, Î¸kl, Ï•kl, âˆ‘l, âˆ‘l_j])
            ccount += 1
        end
        vk = E[k] + âˆ‘l
        RK = RK + vk/D[k, m]
        #println([E[k], âˆ‘l, D[k, m], RK])
    end
    #println(SK)
    VK = RK/SK
    Î±j = D[j, m]*SK - 1.
    Vj = E[j] + âˆ‘l_j
    #println([VK, Vj, Î±j])
    if return_vk
        return (VK - Vj)/Î±j, VK
    else
        return (VK - Vj)/Î±j
    end
end


"""
overloader for Î”jK, use precomputed distance matrix D and SK[m] 
"""
function comp_v_jm(W, E, D, Î¸, Ï•, dÏ•, SK, Midx, n_l, n_feature, m, j)
    RK = 0.
    âˆ‘l_j = 0. # for j indexer, only passed once and j âˆˆ K
    ccount = 1 # the col vector count, should follow k*l, easier to track than trying to compute the indexing pattern.
    for k âˆˆ Midx
        âˆ‘l = 0. # right term with l index
        for l âˆˆ 1:n_l # âˆ‘Î¸_kl*Ï•_kl
            # for k:
            Ï•kl = qÏ•(Ï•, dÏ•, W, m, k, l, n_feature)
            Î¸kl = Î¸[ccount] # since Î¸ is in block vector of [k,l]
            Î¸Ï• = Î¸kl*Ï•kl
            âˆ‘l = âˆ‘l + Î¸Ï•
            if k == j # for j terms:
                âˆ‘l_j = âˆ‘l_j + Î¸Ï•
            end
            #println([ccount, Î¸kl, Ï•kl, âˆ‘l, âˆ‘l_j])
            ccount += 1
        end
        vk = E[k] + âˆ‘l
        RK = RK + vk/D[k, m]
        #println([E[k], âˆ‘l, D[k, m], RK])
    end
    VK = RK/SK
    Î±j = D[j, m]*SK - 1.
    Vj = E[j] + âˆ‘l_j
    #println([VK, Vj, Î±j])
    return (VK - Vj)/Î±j
end


"""
computes the A*x := âˆ‘_{kl} Î¸_kl Ï•_kl (1 - Î³_k Î´_jk)/Î³_k Î±_j
Same as v_j function but for VK only
"""
function comp_Ax_j!(temps, Î¸, B, Midx, cidx, klidx, Î³, Î±, j, jc)
    âˆ‘k, num, den = temps;
    @simd for c âˆˆ cidx  # vectorized op on N vector length s.t. x = [m1, m2, ... N]
        k = Midx[c]
        num .= (@view B[:,klidx[c]])*Î¸[klidx[c]] .* (1 .- (@view Î³[:,c]).*Î´(j,k))
        den .= (@view Î³[:,c]) .* (@view Î±[:, jc])
        âˆ‘k .+= (num ./ den)
        #âˆ‘k .+= /( (@view Î³[:,k]) .* (@view Î±[:, j])) # of length N
    end
end

function comp_Ax!(Ax, Axtemp, temps, Î¸, B, Midx, cidx, klidx, Î³, Î±)
    # loop for all j:
    for jc âˆˆ cidx
        comp_Ax_j!(temps, Î¸, B, Midx, cidx, klidx, Î³, Î±, Midx[jc], jc)
        Axtemp[:, jc] .= temps[1]
        fill!.(temps, 0.)
    end
    Ax .= vec(transpose(Axtemp)) # transpose first for j as inner index then m outer
end

"""
computes b_j := (E_j - âˆ‘_k E_k/Î³_k Î±_j) âˆ€m for each j, 
"""
function comp_b_j!(temps, E, Î³, Î±, Midx, cidx, j, jc)
    b_j, âˆ‘k = temps;
    for c âˆˆ cidx
        k = Midx[c]
        @. âˆ‘k = âˆ‘k + (E[k] / (@view Î³[:, c])) # âˆ‘_k E_k/Î³_k(w_m) , E has absolute index (j, k) while the others are relative indices (jc, mc)
    end
    @. b_j = (E[j] - âˆ‘k) / (@view Î±[:, jc])
end

function comp_b!(b, btemp, temps, E, Î³, Î±, Midx, cidx)
    for jc âˆˆ cidx
        comp_b_j!(temps, E, Î³, Î±, Midx, cidx, Midx[jc], jc)
        btemp[:, jc] .= temps[1]
        fill!.(temps, 0.)
    end
    b .= vec(transpose(btemp))  
end

"""
computes Aáµ€v, where v âˆˆ Float64(col of A), required for CGLS
params:
    
"""
function comp_Aáµ€v!(Aáµ€v, v, B, Midx, Widx, Î³, Î±, L)
    rc = 1 # row counter
    for kc âˆˆ eachindex(Midx)
        k = Midx[kc]; # absolute index k
        for l âˆˆ 1:L
            cc = 1 # col counter
            âˆ‘ = 0.
            for mc âˆˆ eachindex(Widx)
                for jc âˆˆ eachindex(Midx)
                    j = Midx[jc]; # absolute index j
                    num = B[mc, rc]*v[cc]*(1 - Î³[mc,kc]*Î´(j,k))
                    den = Î³[mc, kc]*Î±[mc, jc]
                    âˆ‘ = âˆ‘ + num/den
                    cc += 1
                end
            end
            Aáµ€v[rc] = âˆ‘
            rc += 1
        end
    end
end

"""
computes Î”jK := Î”jK for m = 1,...,N (returns a vector with length N), with precomputed vector of matrices B instead of (W, Ï•, dÏ•)
params:
    - outs, temporary vectors to avoid memalloc
    - E, energy vector, âˆˆ Float64(n_data)
    - D, distance matrix, âˆˆ Float64(n_data, n_data)
    - Î¸, tuning param vec, âˆˆ Float64(M*L)
    - B, matrix containing Ï•_{m,kl}, âˆˆ Float64(N, M*L)
    - SKs, vector containing SK âˆ€m, âˆˆ Float64(N)
    - Midx, vector containing index of supervised data, âˆˆ Int(M)
    - Widx, vector containing index of unsupervised data âˆˆ Int(N)
    - cidx, indexer of k or j, âˆˆ UnitRange(1:M)
    - klidx, vector containing indexer of block column, âˆˆ UnitRange(M, 1:L) 
    - Î±j, vector which contains Î±_j âˆ€m, âˆˆ Float64(N)
    - j, absolute index of j âˆˆ Midx, Int
output:
    - Î”jK, vector âˆ€m, âˆˆ Float64(N) (element of outs vector)
    - VK, VK(w_m) âˆ€m
"""
function comp_v_j!(outs, E, D, Î¸, B, SKs, Midx, Widx, cidx, klidx, Î±j, j)
    Î”jK, VK, vk, vj, RK, Ï•kl, Ï•jl = outs;
    @simd for c âˆˆ cidx # vectorized op on N vector length s.t. x = [m1, m2, ... N]
        k = Midx[c]
        Ï•kl .= B[:,klidx[c]]*Î¸[klidx[c]]
        @. vk = E[k] + Ï•kl
        @. RK = RK + (vk/D[Widx, c])
        if j == k # for j term
            Ï•jl .= Ï•kl
        end
    end
    @. VK = RK / SKs
    @. vj = E[j] + Ï•jl
    @. Î”jK = (VK - vj) / Î±j
end


"""
full Î”jK computer âˆ€jm, m Ã— j matrix
outputs:
    - vmat, matrix Î”jK(w_m) âˆ€m,j âˆˆ Float64(N, M) (preallocated outside!)
    - VK, vector containing VK(w_m) âˆ€m
"""
function comp_v!(v, vmat, VK, outs, E, D, Î¸, B, SKs, Midx, Widx, cidx, klidx, Î±)
    # initial loop for VK (the first one cfant be parallel):
    jc = cidx[1]
    comp_v_j!(outs, E, D, Î¸, B, SKs, Midx, Widx, cidx, klidx, Î±[:, jc], Midx[jc])
    vmat[:, jc] .= outs[1]
    VK .= outs[2] # this only needs to be computed once
    fill!.(outs, 0.)
    # rest of the loop for Î”jK:
    for jc âˆˆ cidx[2:end]
        comp_v_j!(outs, E, D, Î¸, B, SKs, Midx, Widx, cidx, klidx, Î±[:, jc], Midx[jc])
        vmat[:, jc] .= outs[1]
        fill!.(outs, 0.)
    end
    v .= vec(transpose(vmat))
end

"""
only for VK(w_m) prediction
"""
function comp_VK!(VK, outs, E, D, Î¸, B, SKs, Midx, Widx, cidx, klidx)
    vk, RK, Ï•kl = outs;
    @simd for c âˆˆ cidx # vectorized op on N vector length s.t. x = [m1, m2, ... N]
        k = Midx[c]
        Ï•kl .= B[:,klidx[c]]*Î¸[klidx[c]]
        @. vk = E[k] + Ï•kl
        @. RK = RK + (vk/D[Widx, c])
    end
    @. VK = RK / SKs
end

"""
computes the Î”jK across all m âˆˆ T (vectorized across m)
"""
function comp_Î”jK!(outs, VK, E, Î¸, B, klidx, Î±j, jc, j)
    Î”jK, vj, Ï•jl = outs;
    Ï•jl .= B[:,klidx[jc]]*Î¸[klidx[jc]]
    @. vj = E[j] + Ï•jl
    @. Î”jK = (VK - vj) / Î±j
end

"""
computes all of the Î”jK (residuals) given VK for j âˆˆ K, m âˆˆ T, indexed by j first then m
"""
function comp_res!(v, vmat, outs, VK, E, Î¸, B, klidx, Midx, Î±)
    @simd for jc âˆˆ eachindex(Midx)
        j = Midx[jc]
        comp_Î”jK!(outs, VK, E, Î¸, B, klidx, Î±[:, jc], jc, j)
        vmat[:, jc] .= outs[1]
        fill!.(outs, 0.)
    end
    v .= vec(transpose(vmat))
end

"""
MAD_k(w_m) := 1/|K| âˆ‘_jâˆˆK |Î”jK(w_m)| 
"""
function MAD(W, E, D, Î¸, Ï•, dÏ•, Midx, n_l, n_feature, m)
    len = length(Midx)
    âˆ‘ = 0.
    for j âˆˆ Midx
        âˆ‘ = âˆ‘ + abs(comp_Î”jK(W, E, D, Î¸, Ï•, dÏ•, Midx, n_l, n_feature, m, j))
    end
    return âˆ‘/len
end

"""
only from a list of Î”jK
"""
function MAD(Î”jKs)
    len = length(Î”jKs)
    return sum(abs.(Î”jKs))/len
end

"""
specialized distances
"""
function fcenterdist(F, T)
    D = zeros(size(F, 1), length(T))
    for j âˆˆ axes(D, 2)
        for i âˆˆ axes(D, 1)
            D[i, j] = norm((@view F[i,:]) - (@view F[T[j], :]), 2)^2
        end
    end
    return D
end




"""
========================================================================================================================================================================================================================
intermediate engines
========================================================================================================================================================================================================================
"""

"""
main fitter function, assemble LS -> fit -> save to file
to avoid clutter in main function, called within fitting iters

outputs:
    - indexes of n-maximum MAD
"""
function fitter(F, E, D, Ï•, dÏ•, Midx, Tidx, Uidx, Widx, n_feature, mol_name, bsize, tlimit; get_mad=false, get_rmse=false, force=true)
    N = length(Tidx); nU = length(Uidx); nK = length(Midx); Nqm9 = length(Widx)
    nL = size(Ï•, 1); n_basis = nL/n_feature
    #println("[Nqm9, N, nK, nf, ns, nL] = ", [Nqm9, N, nK, n_feature, n_basis, nL])   

    # !!!! using LinearOperators !!!:
    # precompute stuffs:
    t_ab = @elapsed begin
        # indexers:
        klidx = kl_indexer(nK, nL)
        cidx = 1:nK
        # intermediate value:
        SKs_train = map(m -> comp_SK(D, Midx, m), Uidx) # only for training, disjoint index from pred
        Î³ = comp_Î³(D, SKs_train, Midx, Uidx)
        SKs = map(m -> comp_SK(D, Midx, m), Widx) # for prediction
        Î± = Î³ .- 1
        B = comp_Bpar(Ï•, dÏ•, F, Midx, Uidx, nL, n_feature; force=force) #B = zeros(nU, nK*nL); comp_B!(B, Ï•, dÏ•, F, Midx, Uidx, nL, n_feature);
    end
    #println("precomputation time = ",t_ab)
    row = nU*nK; col = nK*nL #define LinearOperator's size
    t_ls = @elapsed begin
        # generate LinOp in place of A!:
        Axtemp = zeros(nU, nK); tempsA = [zeros(nU) for _ in 1:3]
        op = LinearOperator(Float64, row, col, false, false, (y,u) -> comp_Ax!(y, Axtemp, tempsA, u, B, Midx, cidx, klidx, Î³, Î±), 
                                                            (y,v) -> comp_Aáµ€v!(y, v, B, Midx, Uidx, Î³, Î±, nL))
        # show(op)
        # generate b:
        b = zeros(nU*nK); btemp = zeros(nU, nK); tempsb = [zeros(nU) for _ in 1:2]
        comp_b!(b, btemp, tempsb, E, Î³, Î±, Midx, cidx)
        # do LS:
        start = time()
        Î¸, stat = cgls(op, b, itmax=500, verbose=0, callback=CglsSolver -> time_callback(CglsSolver, start, tlimit)) # with callback ðŸŒ¸
        #Î¸, stat = cgls(op, b, itmax=500, verbose=0) # without ccallback
    end

    # get residual:
    obj = norm(op*Î¸ - b)^2
    #println("solver obj = ",obj, ", solver time = ",t_ls)

    # get residuals of training set:
    VK = zeros(nU); outs = [zeros(nU) for _ = 1:3]
    comp_VK!(VK, outs, E, D, Î¸, B, SKs_train, Midx, Uidx, cidx, klidx)
    v = zeros(nU*nK); vmat = zeros(nU, nK); fill!.(outs, 0.)
    comp_res!(v, vmat, outs, VK, E, Î¸, B, klidx, Midx, Î±)
    MADs = vec(sum(abs.(vmat), dims=2)) ./ nK # length nU

    # semi-BATCHMODE PRED for Nqm9:
    blength = Nqm9 Ã· bsize # number of batch iterations
    batches = kl_indexer(blength, bsize)
    bend = batches[end][end]
    bendsize = Nqm9 - (blength*bsize)
    push!(batches, bend+1 : bend + bendsize)
    # compute predictions:
    t_batch = @elapsed begin
        VK_fin = zeros(Nqm9)
        #B = zeros(Float64, bsize, nK*nL)
        VK = zeros(bsize); outs = [zeros(bsize) for _ = 1:3]
        @simd for batch in batches[1:end-1]
            B = comp_Bpar(Ï•, dÏ•, F, Midx, Widx[batch], nL, n_feature; force=force) #comp_B!(B, Ï•, dÏ•, F, Midx, Widx[batch], nL, n_feature)
            comp_VK!(VK, outs, E, D, Î¸, B, SKs[batch], Midx, Widx[batch], cidx, klidx)
            VK_fin[batch] .= VK
            # reset:
            #fill!(B, 0.); 
            fill!(VK, 0.); fill!.(outs, 0.); 
        end
        # remainder part:
        #B = zeros(Float64, bendsize, nK*nL)
        VK = zeros(bendsize); outs = [zeros(bendsize) for _ = 1:3]
        B = comp_Bpar(Ï•, dÏ•, F, Midx, Widx[batches[end]], nL, n_feature; force=force) #comp_B!(B, Ï•, dÏ•, F, Midx, Widx[batches[end]], nL, n_feature)
        comp_VK!(VK, outs, E, D, Î¸, B, SKs[batches[end]], Midx, Widx[batches[end]], cidx, klidx)
        VK_fin[batches[end]] .= VK
        VK = VK_fin # swap
    end
    #println("batchpred time = ",t_batch)

    # get errors: 
    MAE = sum(abs.(VK .- E[Widx])) / Nqm9
    MAE *= 627.503 # convert from Hartree to kcal/mol
    #println("MAE of all mol w/ unknown E is ", MAE)

    RMSE = sqrt(sum((VK .- E[Widx]).^2)/Nqm9) # in original unit, NOT in kcal/mol

    # get the n-highest MAD:
    #= n = 1 # ðŸŒ¸
    sidxes = sortperm(MADs)[end-(n-1):end]
    MADmax_idxes = Widx[sidxes] # the indexes relative to Widx (global data index) =#
    
    # get min |K| RMSD (the obj func):
    RMSD = obj #Optim.minimum(res)
    
    #println("largest MAD is = ", MADs[sidxes[end]], ", with index = ",MADmax_idxes)
    #println("|K|*âˆ‘RMSD(w) = ", RMSD)

    # save also the nK indices and Î¸'s to file!!:
    #data = Dict("centers"=>Midx, "theta"=>Î¸)
    #save("result/$mol_name/theta_center_$mol_name"*"_$matsize.jld", "data", data)
    # clear variables:
    SKs_train = SKs = Î³ = Î± = B = klidx = cidx = Axtemp = tempsA = op = b = tempsb = Î¸ = stat = VK = outs = v = vmat = MADs = batches = VK_fin = nothing; GC.gc()
    # collect the RMS(D,E), D for train, E for test:
    if get_rmse
        return MAE, RMSE, RMSD, t_ls, t_batch
    else
        return MAE, RMSD, t_ls, t_batch #return MAE, MADmax_idxes, t_ls, t_batch
    end
end


"""
converts distance to r^k/(r^k+r0^k), k=1,2,3, r0 â‰ˆ req
"""
function rdist(r,r0;c=1)
    return r^c/(r^c + r0^c)
end

"""
converts distance to e^{-r/r0}
"""
function edist(r,r0)
    return exp(-r/r0)
end



"""
rosemi wrapper fitter, can choose either with kfold or usequence (if kfold = true then pcs isnt used)   
    - F = feature vector (or matrix)
    - E = target energy vector
    - kfold = use kfold or usequence
    - k = number of folds
    - pcs = percentage of centers
    - ptr = percentage of trainings
"""
function rosemi_fitter(F, E, folds; pcs = 0.8, ptr = 0.5, n_basis=4, Î» = 0., force=true)
    ndata = length(E)
    #folds = collect(Kfold(ndata, k)) # for rosemi, these guys are centers, not "training set"
    #println(folds)
    MAEs = []; RMSEs = []; RMSDs = []; t_lss = []; t_preds = []
    for (i,fold) in enumerate(folds)
        # fitter:
        Ï•, dÏ• = extract_bspline_df(F', n_basis; flatten=true, sparsemat=true)
        #fold = shuffle(fold); 
        centers = fold; lenctr = length(centers) # the center ids
        trids = fold[1:Int(round(ptr*lenctr))] # train ids (K)
        uids = setdiff(fold, trids) # unsupervised ids (U)
        tsids = setdiff(1:ndata, fold)
        D = fcenterdist(F, centers) .+ Î» # temp fix for numericals stability
        bsize = max(1, Int(round(0.25*length(tsids)))) # to avoid bsize=0
        MAE, RMSE, RMSD, t_ls, t_pred = fitter(F', E, D, Ï•, dÏ•, trids, centers, uids, tsids, size(F, 2), "test", bsize, 900, get_rmse=true, force=force)    
        MAE = MAE/627.503 ## MAE in energy input's unit
        # store results:
        push!(MAEs, MAE); push!(RMSEs, RMSE); push!(RMSDs, RMSD); push!(t_lss, t_ls); push!(t_preds, t_pred); 
    end
    return MAEs, RMSEs, RMSDs, t_lss, t_preds 
end


"""
rerun of HxOy using ROSEMi
"""
function main_rosemi_hxoy(;force=true, c=1, n_basis=4, ptr=0.5)
    Random.seed!(603)
    # pair HxOy fitting:
    data = load("data/smallmol/hxoy_data_req.jld", "data") # load hxoy
    # do fitting for each dataset:
    # for each dataset split kfold
    # possibly rerun with other models?
    ld_res = []
    for i âˆˆ eachindex(data)[3:3]
        # extract data:
        d = data[i]
        req = d["req"]
        F = rdist.(d["R"], req, c=c) #edist.(d["R"], req) # convert distance features
        E = d["V"]
        println([d["mol"], d["author"], d["state"]])
        folds = shuffle.(collect(Kfold(length(d["V"]), 5))) # do 5-folds here
        MAEs, RMSEs, RMSDs, t_lss, t_preds  = rosemi_fitter(F, E, folds; n_basis=n_basis, ptr=ptr, force=force)
        display([MAEs, RMSEs, RMSDs, t_lss, t_preds ])
        println("RMSE = ", RMSEs)
        # result storage:
        d_res = Dict()
        d_res["MAE"] = MAEs; d_res["RMSE"] = RMSEs; d_res["RMSD"] = RMSDs; d_res["t_train"] = t_lss; d_res["t_test"] = t_preds;
        d_res["mol"] = d["mol"]; d_res["author"] = d["author"]; d_res["state"] = d["state"]; 
        display(d_res)
        push!(ld_res, d_res)
    end
    #save("result/hdrsm_[$force].jld", "data", ld_res)
end


"""
rerun of Hn, 3 â‰¤ n â‰¤ 5 molecules using ROSEMI
"""
function main_rosemi_hn(;force=true, c=1, n_basis=5, ptr=0.6) # the hyperparams are from the optimized H2 Kolos
    Random.seed!(603)
    data = load("data/smallmol/hn_data.jld", "data")
    ld_res = []
    for i âˆˆ setdiff(eachindex(data), [2,3]) # skip partial H4
        Î» = 0.
        d = data[i]; F = rdist.(d["R"], 1.401, c=c); # req of H2  
        E = d["V"]
        if d["mol"] == "H5" # for H5, add "regularizer"
            Î» = 1e-8
        end
        println(d["mol"])
        folds = shuffle.(collect(Kfold(length(d["V"]), 5))) # do 5-folds here
        MAEs, RMSEs, RMSDs, t_lss, t_preds  = rosemi_fitter(F, E, folds; n_basis=n_basis, ptr=ptr, Î» = Î», force=force) 
        display([MAEs, RMSEs, RMSDs, t_lss, t_preds])
        # result storage:
        d_res = Dict()
        d_res["MAE"] = MAEs; d_res["RMSE"] = RMSEs; d_res["RMSD"] = RMSDs; d_res["t_train"] = t_lss; d_res["t_test"] = t_preds;
        d_res["mol"] = d["mol"];
        push!(ld_res, d_res)
    end
    timestamp = replace(replace(string(now()), "-"=>""), ":"=>"")[1:end-4] # exclude ms string
    save("result/hn_rosemi_rerun_$timestamp.jld", "data", ld_res) #save("result/h5_rosemi_rerun_unstable.jld", "data", ld_res) 
end



"""
objective function for hyperparam opt 
"""
function rosemi_fobj(R, E, req, folds; force=true, c=1, n_basis=4, ptr=0.5, Î» = 0.)
    F = rdist.(R, req; c=c)
    MAEs, RMSEs, RMSDs, t_lss, t_preds  = rosemi_fitter(F, E, folds; n_basis=n_basis, ptr=ptr, force=force, Î» = Î»)
    #display(RMSEs)
    return mean(RMSEs) # would mean() be a better metric here? or min() is preferrable?
end